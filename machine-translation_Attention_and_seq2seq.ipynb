{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8634277,"sourceType":"datasetVersion","datasetId":5170288},{"sourceId":8634664,"sourceType":"datasetVersion","datasetId":5170550},{"sourceId":8640937,"sourceType":"datasetVersion","datasetId":5174986}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass MTDataset(Dataset):\n    \"\"\"\n    Create a Dataset class to feed into a torch DataLoader\n    \n    Inputs:\n    - input_matrix: word vectors of input sentences\n    - target_matrix: word vectors of target sentences\n    \n    Return:\n    - pairs of input tensors - target tensors\n    \"\"\"\n    def __init__(self, input_matrix, target_matrix):\n        self.data = []\n        for i in range(len(input_matrix)):\n            self.data.append((input_matrix[i], target_matrix[i]))\n            \n    def __getitem__(self, idx):\n        return (torch.Tensor(self.data[idx][0]), torch.Tensor(self.data[idx][1]))\n    \n    def __len__(self):\n        return len(self.data)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport torch.nn as nn\nimport torch\n\nclass Encoder(nn.Module):\n    \"\"\"\n    Encoder using bi-directional GRU to encode input sentences\n    \n    Arguments:\n    - vocab_size, embedding_dim, hidden_size: integers\n    - modified: False to use Encoder's last backward hidden state like in original paper. True to use a combination between forward and backward hidden states\n    \n    Inputs:\n    - x: batch of input sentences after converting to indices, size (batch, Tx)\n    \n    Returns:\n    - out: output from GRU\n    - last_backward_hidden, last_forward_hidden: hidden states of backward and forward GRU at the last word of input sentences\n    \"\"\"\n    def __init__(self, vocab_size, embedding_dim, hidden_size, modified=False):\n        super(Encoder, self).__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_size = hidden_size\n        self.modified = modified\n        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n        self.gru = nn.GRU(self.embedding_dim, self.hidden_size, batch_first=True, bidirectional=True)\n        if self.modified:\n            self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size)\n        \n    def forward(self, x):\n        embedding = self.embedding(x)\n        out, hidden = self.gru(embedding)\n        last_backward_hidden = out[:, 0, self.hidden_size:].unsqueeze(0)\n        last_forward_hidden = hidden[0].unsqueeze(0)\n        if self.modified:\n            enc_hidden = self.fc_hidden(torch.cat((last_backward_hidden, last_forward_hidden), dim=-1))\n        else:\n            enc_hidden = last_backward_hidden\n        return out, enc_hidden\n    \nclass Decoder(nn.Module):\n    \"\"\"\n    Decoder with Attention for 1 timestep.\n    \n    Arguments:\n    - hidden_size, vocab_size, embedding_dim: integers\n    \n    Inputs:\n    - dec_input: current input\n    - hidden: hidden state from previous timestep\n    - enc_out: output from Encoder\n    \n    Returns:\n    - out: Decoder's output\n    - hidden: hidden states to feed into next timestep's Decoder\n    \"\"\"\n    def __init__(self, hidden_size, vocab_size, embedding_dim):\n        super(Decoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        \n        # Alignment model\n        self.Wa = nn.Linear(self.hidden_size, self.hidden_size)\n        self.Ua = nn.Linear(self.hidden_size * 2, self.hidden_size)\n        self.Va = nn.Linear(self.hidden_size, 1)\n        self.softmax = nn.Softmax(dim=1)\n        \n        # GRU layer\n        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n        self.gru = nn.GRU(self.embedding_dim + self.hidden_size * 2, self.hidden_size, batch_first=True)\n        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n    \n    def forward(self, dec_input, hidden, enc_out):\n        Tx = enc_out.shape[1]\n        hidden_repeat = hidden.permute(1, 0, 2).repeat(1, Tx, 1)\n        energies = self.Va(torch.tanh(self.Wa(hidden_repeat) + self.Ua(enc_out)))\n        alphas = self.softmax(energies)\n        context = torch.sum(alphas * enc_out, dim=1).unsqueeze(1)\n        embedding = self.embedding(dec_input.unsqueeze(1))\n        gru_input = torch.cat((embedding, context), dim=-1)\n        out, hidden = self.gru(gru_input, hidden.contiguous())\n        out = self.out(out)\n        return out, hidden","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:29:49.996044Z","iopub.execute_input":"2024-06-08T15:29:49.996397Z","iopub.status.idle":"2024-06-08T15:29:50.013158Z","shell.execute_reply.started":"2024-06-08T15:29:49.996369Z","shell.execute_reply":"2024-06-08T15:29:50.011920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nimport numpy as np\nimport random\n\nexclude = list(string.punctuation) + list(string.digits)\n\nclass Language(object):\n    \"\"\"\n    Create a language class that contains necessary attributes.\n    \n    Inputs:\n    - sentence_list: a list containing all sentences (string format)\n    - train: True if used for training phase, otherwise False\n    - word2id, id2word: get word2id and id2word from existing training set. Only used for val/test set (train = False), ignored if train = True.\n    \n    Returns a class that contains:\n    - max_len: length of longest sentence in the list\n    - sentences: list containing all sentences\n    - word2id, id2word\n    - vocab_size: number of words after preprocessing\n    - wordvec: word vectors\n    \"\"\"\n    def __init__(self, sentence_list, train=True, word2id=None, id2word=None):\n        self.word2id = word2id\n        self.id2word = id2word\n        self.train = train\n        self.preprocess(sentence_list)\n        self.get_vocab()\n        self.get_word_vectors()\n        \n    def preprocess(self, sentence_list):\n        \"\"\"\n        Preprocess sentences by adding <START> and <END> tokens, then padding all sentences to the same length with <PAD> tokens.\n        \"\"\"\n        self.max_len = 0\n        self.sentences = []\n        for sen in sentence_list:\n            sen = '<START> ' + sen + ' <END>'\n            length = len(sen.split())\n            self.sentences.append(sen)\n            if self.max_len < length:\n                self.max_len = length\n        self.padding()\n    \n    def padding(self):\n        \"\"\"\n        Extend all sentences to the same size by adding <PAD> tokens.\n        \"\"\"\n        for i, sen in enumerate(self.sentences):\n            length = len(sen.split())\n            diff = self.max_len - length\n            paddings = [' <PAD>'] * diff\n            self.sentences[i] = sen + ''.join(paddings)\n            \n    def get_vocab(self):\n        \"\"\"\n        Retrieve word2id, id2word, vocab size.\n        \"\"\"\n        if self.train:\n            self.word2id = {}\n            self.id2word = []\n            for s in self.sentences:\n                for char in s.split():\n                    if char not in self.word2id:\n                        self.id2word.append(char)\n                        self.word2id[char] = len(self.id2word) - 1\n        self.vocab_size = len(self.id2word)\n        \n    def get_word_vectors(self):\n        \"\"\"\n        Retrieve word vectors.\n        \"\"\"\n        self.wordvec = []\n        for i, sen in enumerate(self.sentences):\n            id_list = []\n            for s in sen.split():\n                if s in self.word2id:\n                    id_list.append(self.word2id[s])\n                else:\n                    id_list.append(random.randint(0, self.vocab_size-1))\n            self.wordvec.append(id_list)\n        self.wordvec = np.array(self.wordvec)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:29:58.823104Z","iopub.execute_input":"2024-06-08T15:29:58.823827Z","iopub.status.idle":"2024-06-08T15:29:58.839769Z","shell.execute_reply.started":"2024-06-08T15:29:58.823789Z","shell.execute_reply":"2024-06-08T15:29:58.838713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nimport random\nimport numpy as np\nimport torch\n\ndef generate_seed(seed):\n    \"\"\"\n    Generate a seed for deterministic random calculation.\n    \n    Input:\n    - seed: an integer for seed\n    \"\"\"\n    torch.cuda.manual_seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n\ndef preprocess(inp_filename, target_filename, max_len):\n    with open(inp_filename, 'r', encoding='utf8') as f_inp:\n        lines_inp = f_inp.read().split('\\n')\n    with open(target_filename, 'r', encoding='utf8') as f_trg:\n        lines_trg = f_trg.read().split('\\n')\n    \n    sentences_inp, sentences_trg = [], []\n    exclude = list(string.punctuation) + list(string.digits)\n    \n    for sen_inp, sen_trg in zip(lines_inp, lines_trg):\n        sen_inp = ''.join([char for char in sen_inp if char not in exclude]).strip().lower()\n        sen_trg = ''.join([char for char in sen_trg if char not in exclude]).strip().lower()\n        len_inp = len(sen_inp.split())\n        len_trg = len(sen_trg.split())\n        if len_inp <= max_len and len_trg <= max_len:\n            sentences_inp.append(sen_inp)\n            sentences_trg.append(sen_trg)\n    f_inp.close()\n    f_trg.close()\n    return sentences_inp, sentences_trg","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:30:02.735318Z","iopub.execute_input":"2024-06-08T15:30:02.736005Z","iopub.status.idle":"2024-06-08T15:30:02.746300Z","shell.execute_reply.started":"2024-06-08T15:30:02.735973Z","shell.execute_reply":"2024-06-08T15:30:02.745334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom nltk.translate.bleu_score import corpus_bleu\n\ndef validate(loader, encoder, decoder, id2word, device='cpu'):\n    \"\"\"\n    Validate model's performance on validation set using BLEU-4 score.\n    \n    Inputs:\n    - val_loader: DataLoader for validation set\n    - encoder: an Encoder model\n    - decoder: a Decoder model\n    - id2word: id2word from target training set\n    - device: 'cpu' or 'cuda'\n    \n    Return:\n    - bleu: BLEU-4 score of dataset\n    \"\"\"\n    encoder.eval()\n    decoder.eval()\n    references, hypotheses = [], []\n    with torch.no_grad():\n        for i, (x, y) in enumerate(loader):\n            x = x.to(device=device, dtype=torch.long)\n            y = y.to(device=device, dtype=torch.long)\n            enc_out, enc_hidden = encoder(x)\n            dec_hidden = enc_hidden\n            dec_input = y[:, 0]\n            ref_matrix = y.clone().cpu().numpy()\n            for vec in ref_matrix:\n                sentence = [id2word[id] for id in vec[1:] if id2word[id] not in ['<END>', '<PAD>']]\n                references.append([sentence])\n            hypo_matrix = []\n            for t in range(1, y.size(1)):\n                out, dec_hidden = decoder(dec_input, dec_hidden, enc_out)\n                top = torch.max(out, dim=-1)[1].squeeze(1)\n                dec_input = top\n                next_id = list(top.clone().cpu().numpy())\n                hypo_matrix.append(next_id)\n            hypo_matrix = np.array(hypo_matrix).transpose()\n            for vec in hypo_matrix:\n                sentence = [id2word[id] for id in vec if id2word[id] not in ['<END>', '<PAD>']]\n                hypotheses.append(sentence)\n        bleu = corpus_bleu(list_of_references=references, hypotheses=hypotheses)\n        encoder.train()\n        decoder.train()\n        return bleu","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:30:06.586838Z","iopub.execute_input":"2024-06-08T15:30:06.587580Z","iopub.status.idle":"2024-06-08T15:30:07.745369Z","shell.execute_reply.started":"2024-06-08T15:30:06.587546Z","shell.execute_reply":"2024-06-08T15:30:07.744390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef train(encoder, decoder, train_loader, val_loader, optimizer, criterion, id2word, lr_scheduler=None, num_epochs=1, print_every=100, device='cpu', early_stop=False):\n    \"\"\"\n    Function for training\n    \n    Inputs:\n    - encoder, decoder\n    - train_loader, val_loader: DataLoader for training set and validation set\n    - optimizer: a torch.optim optimizer (e.g. torch.optim.Adam(...))\n    - criterion: loss function (e.g. nn.CrossEntropyLoss())\n    - id2word: id2word for target training set\n    - lr_scheduler: learning rate scheduler (e.g. torch.optim.lr_scheduler.StepLR)\n    - num_epochs\n    - print_every\n    - device: 'cpu' or 'cuda'\n    \"\"\"\n    encoder.train()\n    decoder.train()\n    best_bleu = 0\n    best_statedict = {'encoder': encoder.state_dict(), 'decoder': decoder.state_dict()}\n    for epoch in range(num_epochs):\n        print('Epoch ', epoch + 1)\n        for i, (x, y) in enumerate(train_loader):\n            x = x.to(device=device, dtype=torch.long)\n            y = y.to(device=device, dtype=torch.long)\n            enc_out, enc_hidden = encoder(x)\n            dec_hidden = enc_hidden\n            dec_input = y[:, 0]\n            loss = 0\n            optimizer.zero_grad()\n            for t in range(1, y.size(1)):\n                out, dec_hidden = decoder(dec_input, dec_hidden, enc_out)\n                dec_input = y[:, t]\n                loss += criterion(out.squeeze(1), y[:, t])\n            loss.backward()\n            optimizer.step()\n            if i % print_every == 0:\n                print('Iter %d, loss = %f' %(i, loss.item() / y.size(1)))\n        if lr_scheduler != None:\n            lr_scheduler.step()\n        bleu = validate(val_loader, encoder, decoder, id2word, device)\n        print('Validation BLEU score: %f\\n' %bleu)\n        if bleu > best_bleu:\n            best_statedict = {'encoder': encoder.state_dict(), 'decoder': decoder.state_dict()}\n            best_bleu = bleu\n        elif early_stop:\n            print('=== BLEU begins to decrease, training exits ===')\n            return best_statedict\n    return best_statedict","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:30:12.934549Z","iopub.execute_input":"2024-06-08T15:30:12.935410Z","iopub.status.idle":"2024-06-08T15:30:12.950406Z","shell.execute_reply.started":"2024-06-08T15:30:12.935357Z","shell.execute_reply":"2024-06-08T15:30:12.949509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nimport torch\n\ndef translate(sentence, inp_word2id, trg_word2id, trg_id2word, encoder, decoder, trg_max_len, device='cpu'):\n    \"\"\"\n    Generate translation for input sentence.\n    \n    Inputs:\n    - sentence: a sentence in string format\n    - inp_word2id: word2id from input training set\n    - trg_word2id: word2id from target training set\n    - trg_id2word: id2word from target training set\n    - encoder, decoder: Encoder, Decoder models\n    - trg_max_len: max length for target sentence\n    - device: 'cpu' or 'cuda'\n    \n    Return a sentence\n    \"\"\"\n    exclude = list(string.punctuation) + list(string.digits)\n    sentence = '<START> ' + ''.join([char for char in sentence if char not in exclude]).strip().lower() + ' <END>'\n    sen_matrix = [inp_word2id[s] for s in sentence.split()]\n    sen_tensor = torch.Tensor(sen_matrix).to(device=device, dtype=torch.long).unsqueeze(0)\n    encoder.eval()\n    decoder.eval()\n    with torch.no_grad():\n        enc_out, enc_hidden = encoder(sen_tensor)\n        dec_hidden = enc_hidden\n        dec_input = torch.Tensor([trg_word2id['<START>']]).to(device='cuda', dtype=torch.long)\n        output_list = []\n        for t in range(1, trg_max_len):\n            out, dec_hidden = decoder(dec_input, dec_hidden, enc_out)\n            dec_input = torch.max(out, dim=-1)[1].squeeze(1)\n            next_id = dec_input.squeeze().clone().cpu().numpy()\n            next_word = trg_id2word[next_id]\n            if next_word == '<END>':\n                break\n            output_list.append(next_word)\n        return ' '.join(output_list)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:30:15.433163Z","iopub.execute_input":"2024-06-08T15:30:15.433567Z","iopub.status.idle":"2024-06-08T15:30:15.445720Z","shell.execute_reply.started":"2024-06-08T15:30:15.433535Z","shell.execute_reply":"2024-06-08T15:30:15.444648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 64\nsentences_inp_train, sentences_trg_train = preprocess('/kaggle/input/nlp-dataset/tokenization/train/train.vi','/kaggle/input/nlp-dataset/tokenization/train/train.en',  max_len=MAX_LEN)\nsentences_inp_val, sentences_trg_val = preprocess('/kaggle/input/nlp-dataset/tokenization/dev/dev.vi','/kaggle/input/nlp-dataset/tokenization/dev/dev.en',  max_len=MAX_LEN)\nsentences_inp_test, sentences_trg_test = preprocess('/kaggle/input/nlp-dataset/tokenization/test/test.vi','/kaggle/input/nlp-dataset/tokenization/test/test.en', max_len=MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:30:19.798717Z","iopub.execute_input":"2024-06-08T15:30:19.799563Z","iopub.status.idle":"2024-06-08T15:30:57.439769Z","shell.execute_reply.started":"2024-06-08T15:30:19.799532Z","shell.execute_reply":"2024-06-08T15:30:57.438952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_inp = Language(sentences_inp_train)\ntrain_trg = Language(sentences_trg_train)\n\nval_inp = Language(sentences_inp_val, train=False, word2id=train_inp.word2id, id2word=train_inp.id2word)\nval_trg = Language(sentences_trg_val, train=False, word2id=train_trg.word2id, id2word=train_trg.id2word)\n\ntest_inp = Language(sentences_inp_test, train=False, word2id=train_inp.word2id, id2word=train_inp.id2word)\ntest_trg = Language(sentences_trg_test, train=False, word2id=train_trg.word2id, id2word=train_trg.id2word)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:31:02.690381Z","iopub.execute_input":"2024-06-08T15:31:02.691091Z","iopub.status.idle":"2024-06-08T15:31:17.483776Z","shell.execute_reply.started":"2024-06-08T15:31:02.691057Z","shell.execute_reply":"2024-06-08T15:31:17.482964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = MTDataset(train_inp.wordvec, train_trg.wordvec)\nval_set = MTDataset(val_inp.wordvec, val_trg.wordvec)\ntest_set = MTDataset(test_inp.wordvec, test_trg.wordvec)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:31:22.796988Z","iopub.execute_input":"2024-06-08T15:31:22.797979Z","iopub.status.idle":"2024-06-08T15:31:22.924928Z","shell.execute_reply.started":"2024-06-08T15:31:22.797946Z","shell.execute_reply":"2024-06-08T15:31:22.924151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import StepLR","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:31:26.139671Z","iopub.execute_input":"2024-06-08T15:31:26.140502Z","iopub.status.idle":"2024-06-08T15:31:26.144657Z","shell.execute_reply.started":"2024-06-08T15:31:26.140466Z","shell.execute_reply":"2024-06-08T15:31:26.143707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=64)\ntest_loader = DataLoader(test_set, batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:31:29.059223Z","iopub.execute_input":"2024-06-08T15:31:29.059842Z","iopub.status.idle":"2024-06-08T15:31:29.064962Z","shell.execute_reply.started":"2024-06-08T15:31:29.059809Z","shell.execute_reply":"2024-06-08T15:31:29.063917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tx, Ty = train_inp.max_len, train_trg.max_len\nvocab_size_inp, vocab_size_trg = train_inp.vocab_size, train_trg.vocab_size\nembedding_dim = 256\nhidden_size = 1024","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:31:30.528831Z","iopub.execute_input":"2024-06-08T15:31:30.529176Z","iopub.status.idle":"2024-06-08T15:31:30.533744Z","shell.execute_reply.started":"2024-06-08T15:31:30.529134Z","shell.execute_reply":"2024-06-08T15:31:30.532815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device='cuda'\nelse:\n    device='cpu'","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:31:32.849338Z","iopub.execute_input":"2024-06-08T15:31:32.850264Z","iopub.status.idle":"2024-06-08T15:31:32.908852Z","shell.execute_reply.started":"2024-06-08T15:31:32.850219Z","shell.execute_reply":"2024-06-08T15:31:32.907518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# choose a seed for both models for consistent results\nSEED = 5","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:31:35.376634Z","iopub.execute_input":"2024-06-08T15:31:35.376990Z","iopub.status.idle":"2024-06-08T15:31:35.381410Z","shell.execute_reply.started":"2024-06-08T15:31:35.376962Z","shell.execute_reply":"2024-06-08T15:31:35.380341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_seed(SEED)\nencoder_2 = Encoder(vocab_size_inp, embedding_dim, hidden_size, modified=True).to(device=device)\ndecoder_2 = Decoder(hidden_size, vocab_size_trg, embedding_dim).to(device=device)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:31:36.804607Z","iopub.execute_input":"2024-06-08T15:31:36.805431Z","iopub.status.idle":"2024-06-08T15:31:37.748295Z","shell.execute_reply.started":"2024-06-08T15:31:36.805401Z","shell.execute_reply":"2024-06-08T15:31:37.747481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer_2 = torch.optim.Adam(params=list(encoder_2.parameters()) + list(decoder_2.parameters()))\ncriterion_2 = nn.CrossEntropyLoss()\nscheduler_2 = StepLR(optimizer_2, step_size=2, gamma=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:31:46.144664Z","iopub.execute_input":"2024-06-08T15:31:46.145061Z","iopub.status.idle":"2024-06-08T15:31:47.518371Z","shell.execute_reply.started":"2024-06-08T15:31:46.145031Z","shell.execute_reply":"2024-06-08T15:31:47.517385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train model\nstatedict_2 = train(encoder_2, decoder_2, train_loader, val_loader, optimizer_2, criterion_2, train_trg.id2word, scheduler_2, 20, 200, device)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T17:44:41.488408Z","iopub.execute_input":"2024-06-07T17:44:41.489359Z","iopub.status.idle":"2024-06-07T18:16:34.450629Z","shell.execute_reply.started":"2024-06-07T17:44:41.489326Z","shell.execute_reply":"2024-06-07T18:16:34.449763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save state dict\ntorch.save(statedict_2, 'statedict_2.pth')  #  save model's state dict","metadata":{"execution":{"iopub.status.busy":"2024-06-07T18:27:54.818765Z","iopub.execute_input":"2024-06-07T18:27:54.819205Z","iopub.status.idle":"2024-06-07T18:27:55.153560Z","shell.execute_reply.started":"2024-06-07T18:27:54.819174Z","shell.execute_reply":"2024-06-07T18:27:55.152609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load state dict\nstatedict_2 = torch.load('statedict_2.pth')\nencoder_2.load_state_dict(statedict_2['encoder'])\ndecoder_2.load_state_dict(statedict_2['decoder'])","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:40:17.180837Z","iopub.execute_input":"2024-06-08T15:40:17.181208Z","iopub.status.idle":"2024-06-08T15:40:17.372271Z","shell.execute_reply.started":"2024-06-08T15:40:17.181176Z","shell.execute_reply":"2024-06-08T15:40:17.371243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Model 2 BLEU score: %.3f' %(100*validate(test_loader, encoder_2, decoder_2, test_trg.id2word, device)))","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:40:24.780158Z","iopub.execute_input":"2024-06-08T15:40:24.780699Z","iopub.status.idle":"2024-06-08T15:43:32.024496Z","shell.execute_reply.started":"2024-06-08T15:40:24.780669Z","shell.execute_reply":"2024-06-08T15:43:32.023501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence = \"anh quốc và mỹ nhiều nước khác nữa nó cũng là nước khác và những nước khác nữa\"\nprint(\"Sentence: \" + sentence)\n\nprint(\"Model 2: \" + translate(sentence, train_inp.word2id, train_trg.word2id, train_trg.id2word, encoder_2, decoder_2, MAX_LEN, device))","metadata":{"execution":{"iopub.status.busy":"2024-06-08T15:53:24.508635Z","iopub.execute_input":"2024-06-08T15:53:24.509372Z","iopub.status.idle":"2024-06-08T15:53:24.598763Z","shell.execute_reply.started":"2024-06-08T15:53:24.509340Z","shell.execute_reply":"2024-06-08T15:53:24.597339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}