{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T09:41:29.199057Z","iopub.status.busy":"2024-06-08T09:41:29.198645Z","iopub.status.idle":"2024-06-08T09:41:47.487717Z","shell.execute_reply":"2024-06-08T09:41:47.486370Z","shell.execute_reply.started":"2024-06-08T09:41:29.199022Z"},"trusted":true},"outputs":[],"source":["!pip install pyvi"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T09:41:47.491916Z","iopub.status.busy":"2024-06-08T09:41:47.491457Z","iopub.status.idle":"2024-06-08T09:42:11.803572Z","shell.execute_reply":"2024-06-08T09:42:11.802294Z","shell.execute_reply.started":"2024-06-08T09:41:47.491873Z"},"trusted":true},"outputs":[],"source":["# import libs\n","from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n","import torch\n","from torchtext.data.utils import get_tokenizer\n","from collections import Counter\n","from torchtext.vocab import vocab\n","import re\n","from datasets import Dataset, DatasetDict\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","from torchtext.data.metrics import bleu_score\n","from pyvi import ViTokenizer\n","import wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T09:42:11.805376Z","iopub.status.busy":"2024-06-08T09:42:11.804986Z","iopub.status.idle":"2024-06-08T09:42:11.811342Z","shell.execute_reply":"2024-06-08T09:42:11.810058Z","shell.execute_reply.started":"2024-06-08T09:42:11.805332Z"},"trusted":true},"outputs":[],"source":["train_filepaths=[\n","    r'/kaggle/input/pho-mt/train.en',\n","    r'/kaggle/input/pho-mt/train.vi'\n","]\n","dev_filepaths=[\n","    r'/kaggle/input/pho-mt/dev.en',\n","    r'/kaggle/input/pho-mt/dev.vi'\n","]\n","test_filepaths=[\n","    r'/kaggle/input/pho-mt/test.en',\n","    r'/kaggle/input/pho-mt/test.vi'\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T09:42:11.815797Z","iopub.status.busy":"2024-06-08T09:42:11.814446Z","iopub.status.idle":"2024-06-08T09:42:11.837222Z","shell.execute_reply":"2024-06-08T09:42:11.835957Z","shell.execute_reply.started":"2024-06-08T09:42:11.815748Z"},"trusted":true},"outputs":[],"source":["BATCH_SIZE=16\n","lower=True\n","SRC_LANGUAGE = 'en'\n","TGT_LANGUAGE = 'vi'\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","load_model = True"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T09:42:11.839362Z","iopub.status.busy":"2024-06-08T09:42:11.838898Z","iopub.status.idle":"2024-06-08T09:42:15.199356Z","shell.execute_reply":"2024-06-08T09:42:15.197850Z","shell.execute_reply.started":"2024-06-08T09:42:11.839319Z"},"trusted":true},"outputs":[],"source":["def vi_tokenizer(sentence):\n","    tok_trans=ViTokenizer.tokenize(sentence).split()\n","    result=[]\n","    for tok in tok_trans:\n","        result.append(tok.replace('_',' '))\n","    return result\n","\n","token_transform = {\n","    'en': get_tokenizer('spacy', language='en_core_web_sm'),\n","    'vi': vi_tokenizer\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T09:42:15.201953Z","iopub.status.busy":"2024-06-08T09:42:15.200966Z","iopub.status.idle":"2024-06-08T09:42:31.942991Z","shell.execute_reply":"2024-06-08T09:42:31.941513Z","shell.execute_reply.started":"2024-06-08T09:42:15.201906Z"},"trusted":true},"outputs":[],"source":["# Function to load data from files\n","def load_data(en_path, vi_path):\n","    with open(vi_path, encoding='utf-8') as f:\n","        vi_data = f.readlines()\n","    with open(en_path, encoding='utf-8') as f:\n","        en_data = f.readlines()\n","    return {'en': en_data, 'vi': vi_data}\n","\n","# Load train, dev, and test data\n","train_data = load_data(train_filepaths[0], train_filepaths[1])\n","dev_data = load_data(dev_filepaths[0], dev_filepaths[1])\n","test_data = load_data(test_filepaths[0], test_filepaths[1])\n","\n","# Create DatasetDict\n","datasets = DatasetDict({\n","    'train': Dataset.from_dict(train_data),\n","    'validation': Dataset.from_dict(dev_data),\n","    'test': Dataset.from_dict(test_data)\n","})\n","print(datasets)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T09:42:31.945192Z","iopub.status.busy":"2024-06-08T09:42:31.944822Z","iopub.status.idle":"2024-06-08T09:42:31.950992Z","shell.execute_reply":"2024-06-08T09:42:31.949654Z","shell.execute_reply.started":"2024-06-08T09:42:31.945158Z"},"trusted":true},"outputs":[],"source":["# Define special symbols and indices\n","UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n","# Make sure the tokens are in order of their indices to properly insert them in vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T09:42:31.953316Z","iopub.status.busy":"2024-06-08T09:42:31.952934Z","iopub.status.idle":"2024-06-08T09:42:40.945867Z","shell.execute_reply":"2024-06-08T09:42:40.944394Z","shell.execute_reply.started":"2024-06-08T09:42:31.953282Z"},"trusted":true},"outputs":[],"source":["# Load the pretrained ViT5 model\n","from transformers import T5ForConditionalGeneration, T5Tokenizer\n","model_name = \"VietAI/vit5-base\"  # replace with the actual model name if available\n","tokenizer = T5Tokenizer.from_pretrained(\"VietAI/vit5-base\")\n","model = T5ForConditionalGeneration.from_pretrained('/kaggle/input/vit5-base/fine-tuned-vit5')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T09:42:40.948101Z","iopub.status.busy":"2024-06-08T09:42:40.947689Z","iopub.status.idle":"2024-06-08T09:42:42.480354Z","shell.execute_reply":"2024-06-08T09:42:42.479020Z","shell.execute_reply.started":"2024-06-08T09:42:40.948067Z"},"trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","\n","# Function to sample a fraction of the dataset\n","def sample_dataset(dataset, fraction=1/500):\n","    return dataset.train_test_split(test_size=(1 - fraction))['train']\n","\n","# Sample train, validation, and test sets\n","sampled_train = sample_dataset(datasets['train'])\n","sampled_validation = sample_dataset(datasets['validation'])\n","sampled_test = datasets['test']\n","\n","# Combine the sampled datasets into a new DatasetDict\n","sampled_dataset = DatasetDict({\n","    'train': sampled_train,\n","    'validation': sampled_validation,\n","    'test': sampled_test\n","})\n","\n","# Display the sampled dataset information\n","print(sampled_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T09:42:42.484438Z","iopub.status.busy":"2024-06-08T09:42:42.484063Z","iopub.status.idle":"2024-06-08T09:42:57.876314Z","shell.execute_reply":"2024-06-08T09:42:57.874924Z","shell.execute_reply.started":"2024-06-08T09:42:42.484403Z"},"trusted":true},"outputs":[],"source":["# Preprocess the dataset\n","def preprocess_function(examples):\n","    inputs = [\"translate English to Vietnamese: \" + ex for ex in examples['en']]\n","    targets = [ex for ex in examples['vi']]\n","    model_inputs = tokenizer(inputs, max_length=64, truncation=True, padding=\"max_length\")\n","    labels = tokenizer(targets, max_length=64, truncation=True, padding=\"max_length\")\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","tokenized_datasets = sampled_dataset.map(preprocess_function, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T09:42:57.878688Z","iopub.status.busy":"2024-06-08T09:42:57.878214Z","iopub.status.idle":"2024-06-08T09:42:57.889338Z","shell.execute_reply":"2024-06-08T09:42:57.887927Z","shell.execute_reply.started":"2024-06-08T09:42:57.878644Z"},"trusted":true},"outputs":[],"source":["# Define function to collate data samples into batch tensors\n","def generate_batch(batch):\n","    src_batch = [sample['input_ids'] for sample in batch]\n","    tgt_batch = [sample['labels'] for sample in batch]\n","    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n","    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n","    return {'input_ids': src_batch, 'labels': tgt_batch}\n","\n","# Create DataLoaders\n","train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n","val_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n","test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T09:42:57.891837Z","iopub.status.busy":"2024-06-08T09:42:57.891351Z","iopub.status.idle":"2024-06-08T09:42:57.905554Z","shell.execute_reply":"2024-06-08T09:42:57.904304Z","shell.execute_reply.started":"2024-06-08T09:42:57.891789Z"},"trusted":true},"outputs":[],"source":["# Data collator\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T09:42:57.907644Z","iopub.status.busy":"2024-06-08T09:42:57.907173Z","iopub.status.idle":"2024-06-08T09:42:59.575747Z","shell.execute_reply":"2024-06-08T09:42:59.573892Z","shell.execute_reply.started":"2024-06-08T09:42:57.907601Z"},"trusted":true},"outputs":[],"source":["!pip install transformers[torch] datasets torch pyvi sacrebleu accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-08T09:42:59.578944Z","iopub.status.busy":"2024-06-08T09:42:59.578466Z","iopub.status.idle":"2024-06-08T09:43:00.576919Z","shell.execute_reply":"2024-06-08T09:43:00.575363Z","shell.execute_reply.started":"2024-06-08T09:42:59.578899Z"},"trusted":true},"outputs":[],"source":["import sacrebleu\n","import numpy as np\n","# Inference functions\n","def infer(text, model, tokenizer):\n","    model.eval()\n","    input_text = \"translate English to Vietnamese: \" + text\n","    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", max_length=128, truncation=True).input_ids\n","\n","    with torch.no_grad():\n","        output_ids = model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)\n","\n","    translated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","    return translated_text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:43:00.578001Z","iopub.status.idle":"2024-06-08T09:43:00.578431Z","shell.execute_reply":"2024-06-08T09:43:00.578246Z","shell.execute_reply.started":"2024-06-08T09:43:00.578227Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(predictions, references):\n","    bleu = sacrebleu.corpus_bleu(predictions, [references])\n","    return bleu.score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:43:00.580210Z","iopub.status.idle":"2024-06-08T09:43:00.580663Z","shell.execute_reply":"2024-06-08T09:43:00.580438Z","shell.execute_reply.started":"2024-06-08T09:43:00.580421Z"},"trusted":true},"outputs":[],"source":["def infer_test_set(model, tokenizer, tokenized_dataset, times, skip_small_text=0, sample=500):\n","    count = 0\n","    bleu = np.array([])\n","    \n","    for text_dict in tokenized_dataset:\n","        if count == sample:\n","            break\n","\n","        text = tokenizer.decode(text_dict[\"input_ids\"], skip_special_tokens=True).replace(\"translate English to Vietnamese: \", \"\")\n","        label = tokenizer.decode(text_dict[\"labels\"], skip_special_tokens=True)\n","\n","        if skip_small_text > 0:\n","            if len(text) < skip_small_text:\n","                continue\n","        count += 1\n","\n","        output_list = []\n","        bleu_score_list = []\n","        \n","        for i in range(times):\n","            output = infer(text, model, tokenizer)\n","            score = compute_metrics([output], [label])\n","            output_list.append(output)\n","            bleu_score_list.append(score)\n","        \n","        max_index = bleu_score_list.index(max(bleu_score_list))\n","        bleu_score = bleu_score_list[max_index]\n","        bleu = np.append(bleu, bleu_score)\n","        \n","        if count % 2000 == 0:\n","            print(\"Bleu Score: \", bleu_score_list[max_index])\n","            print(\"Input: \" + text)\n","            print(\"Prediction: \" + output_list[max_index])\n","            print(\"Label: \" + label)\n","            print()\n","\n","    print(f\"Length: {len(bleu)}\")\n","    if skip_small_text > 0:\n","        print(f\"MEAN BLEU SCORE with loop= {times}, skip all text with length shorter than {skip_small_text}: {bleu.mean()}\")\n","    else:\n","        print(f\"MEAN BLEU SCORE with loop= {times}, do not skip small text: {bleu.mean()}\")\n","    print(\"BLEU SCORE: \", bleu.mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-08T09:43:00.582281Z","iopub.status.idle":"2024-06-08T09:43:00.582809Z","shell.execute_reply":"2024-06-08T09:43:00.582565Z","shell.execute_reply.started":"2024-06-08T09:43:00.582539Z"},"trusted":true},"outputs":[],"source":["infer_test_set(model, tokenizer, tokenized_datasets['test'], times=1, skip_small_text=40, sample=19000)\n","print()\n","print()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5165397,"sourceId":8627588,"sourceType":"datasetVersion"},{"sourceId":182140470,"sourceType":"kernelVersion"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
