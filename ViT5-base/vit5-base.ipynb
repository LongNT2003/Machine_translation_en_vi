{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8627588,"sourceType":"datasetVersion","datasetId":5165397}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Import Library**","metadata":{}},{"cell_type":"code","source":"# import libs\nfrom transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\nimport torch\nfrom torchtext.data.utils import get_tokenizer\nfrom collections import Counter\nfrom torchtext.vocab import vocab\nimport re\nfrom datasets import Dataset, DatasetDict\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\nfrom torchtext.data.metrics import bleu_score\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2024-06-10T03:05:40.974347Z","iopub.execute_input":"2024-06-10T03:05:40.974683Z","iopub.status.idle":"2024-06-10T03:06:14.035381Z","shell.execute_reply.started":"2024-06-10T03:05:40.974657Z","shell.execute_reply":"2024-06-10T03:06:14.034574Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-10 03:05:58.783176: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-10 03:05:58.783304: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-10 03:05:59.035969: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Load dataset**","metadata":{}},{"cell_type":"code","source":"train_filepaths=[\n    r'/kaggle/input/pho-mt/train.en',\n    r'/kaggle/input/pho-mt/train.vi'\n]\ndev_filepaths=[\n    r'/kaggle/input/pho-mt/dev.en',\n    r'/kaggle/input/pho-mt/dev.vi'\n]\ntest_filepaths=[\n    r'/kaggle/input/pho-mt/test.en',\n    r'/kaggle/input/pho-mt/test.vi'\n]","metadata":{"execution":{"iopub.status.busy":"2024-06-10T03:06:14.037253Z","iopub.execute_input":"2024-06-10T03:06:14.037864Z","iopub.status.idle":"2024-06-10T03:06:14.042621Z","shell.execute_reply.started":"2024-06-10T03:06:14.037832Z","shell.execute_reply":"2024-06-10T03:06:14.041674Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE=32\nlower=True\nSRC_LANGUAGE = 'en'\nTGT_LANGUAGE = 'vi'\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nload_model = True\nsave_model = True","metadata":{"execution":{"iopub.status.busy":"2024-06-10T03:06:14.043907Z","iopub.execute_input":"2024-06-10T03:06:14.044219Z","iopub.status.idle":"2024-06-10T03:06:14.099341Z","shell.execute_reply.started":"2024-06-10T03:06:14.044195Z","shell.execute_reply":"2024-06-10T03:06:14.098493Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Function to load data from files\ndef load_data(en_path, vi_path):\n    with open(en_path, encoding='utf-8') as f:\n        en_data = f.readlines()\n    with open(vi_path, encoding='utf-8') as f:\n        vi_data = f.readlines()\n    return {'en': en_data, 'vi': vi_data}\n\n# Load train, dev, and test data\ntrain_data = load_data(train_filepaths[0], train_filepaths[1])\ndev_data = load_data(dev_filepaths[0], dev_filepaths[1])\ntest_data = load_data(test_filepaths[0], test_filepaths[1])\n\n# Create DatasetDict\ndatasets = DatasetDict({\n    'train': Dataset.from_dict(train_data),\n    'validation': Dataset.from_dict(dev_data),\n    'test': Dataset.from_dict(test_data)\n})\nprint(datasets)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T03:06:14.101819Z","iopub.execute_input":"2024-06-10T03:06:14.102122Z","iopub.status.idle":"2024-06-10T03:06:32.354622Z","shell.execute_reply.started":"2024-06-10T03:06:14.102097Z","shell.execute_reply":"2024-06-10T03:06:32.353674Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['en', 'vi'],\n        num_rows: 2977999\n    })\n    validation: Dataset({\n        features: ['en', 'vi'],\n        num_rows: 18719\n    })\n    test: Dataset({\n        features: ['en', 'vi'],\n        num_rows: 19151\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"# Use 1/500 dataset for testing\nfrom datasets import load_dataset\n\n# Function to sample a fraction of the dataset\ndef sample_dataset(dataset, fraction=1/500):\n    return dataset.train_test_split(test_size=(1 - fraction))['train']\n\n# Sample train, validation, and test sets\nsampled_train = sample_dataset(datasets['train'])\nsampled_validation = sample_dataset(datasets['validation'])\nsampled_test = datasets['test']\n\n# Combine the sampled datasets into a new DatasetDict\nsampled_dataset = DatasetDict({\n    'train': sampled_train,\n    'validation': sampled_validation,\n    'test': sampled_test\n})\n\n# Display the sampled dataset information\nprint(sampled_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T03:06:32.355882Z","iopub.execute_input":"2024-06-10T03:06:32.356270Z","iopub.status.idle":"2024-06-10T03:06:33.650435Z","shell.execute_reply.started":"2024-06-10T03:06:32.356216Z","shell.execute_reply":"2024-06-10T03:06:33.649510Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['en', 'vi'],\n        num_rows: 5955\n    })\n    validation: Dataset({\n        features: ['en', 'vi'],\n        num_rows: 37\n    })\n    test: Dataset({\n        features: ['en', 'vi'],\n        num_rows: 19151\n    })\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Import model**","metadata":{}},{"cell_type":"code","source":"# Load the pretrained ViT5 model\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nmodel_name = \"VietAI/vit5-base\"  # replace with the actual model name if available\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T03:06:33.651662Z","iopub.execute_input":"2024-06-10T03:06:33.652027Z","iopub.status.idle":"2024-06-10T03:06:42.691112Z","shell.execute_reply.started":"2024-06-10T03:06:33.651995Z","shell.execute_reply":"2024-06-10T03:06:42.690293Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97c74b161a254ea8a015f9f6bccd7831"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/820k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb773721d29142f5af2fa28a9f048800"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08c97c0ba42e4db6ba80a86190d66985"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.40M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78728244d97143d7853adda8d5b9c53e"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/702 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96964675210a4e4a9018498443ebfeff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/904M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"affe4f7a920842c0b378bf67eb041c24"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"# Preprocess the dataset\ndef preprocess_function(examples):\n    inputs = [\"translate English to Vietnamese: \" + ex for ex in examples['en']]\n    targets = [ex for ex in examples['vi']]\n    model_inputs = tokenizer(inputs, max_length=64, truncation=True, padding=\"max_length\")\n    labels = tokenizer(targets, max_length=64, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_datasets = sampled_dataset.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T03:06:42.692185Z","iopub.execute_input":"2024-06-10T03:06:42.692480Z","iopub.status.idle":"2024-06-10T03:06:55.293547Z","shell.execute_reply.started":"2024-06-10T03:06:42.692456Z","shell.execute_reply":"2024-06-10T03:06:55.292593Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5955 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81a4371314d64fb8bc9a909c8dfe2ec5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/37 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82ac9a9c024f447b9f744c28d2cc004d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19151 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e26974784d747399c2caffd2e8cd18b"}},"metadata":{}}]},{"cell_type":"code","source":"# Define function to collate data samples into batch tensors\ndef generate_batch(batch):\n    src_batch = [sample['input_ids'] for sample in batch]\n    tgt_batch = [sample['labels'] for sample in batch]\n    src_batch = pad_sequence(src_batch, padding_value=1)\n    tgt_batch = pad_sequence(tgt_batch, padding_value=1)\n    return {'input_ids': src_batch, 'labels': tgt_batch}\n\n# Create DataLoaders\ntrain_dataloader = DataLoader(tokenized_datasets['train'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\nval_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\ntest_dataloader = DataLoader(tokenized_datasets['test'], batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T03:06:55.294864Z","iopub.execute_input":"2024-06-10T03:06:55.295250Z","iopub.status.idle":"2024-06-10T03:06:55.304394Z","shell.execute_reply.started":"2024-06-10T03:06:55.295198Z","shell.execute_reply":"2024-06-10T03:06:55.303195Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# **Training**","metadata":{}},{"cell_type":"code","source":"! pip install sacrebleu","metadata":{"execution":{"iopub.status.busy":"2024-06-10T03:07:32.965692Z","iopub.execute_input":"2024-06-10T03:07:32.966392Z","iopub.status.idle":"2024-06-10T03:07:48.215746Z","shell.execute_reply.started":"2024-06-10T03:07:32.966363Z","shell.execute_reply":"2024-06-10T03:07:48.214607Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m915.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.12.25)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.2.1)\nDownloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-2.8.2 sacrebleu-2.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_metric\nimport numpy as np\n\nmetric = load_metric(\"sacrebleu\")\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n\n    return preds, labels\n\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    result = {\"bleu\": result[\"score\"]}\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-06-10T03:07:51.575874Z","iopub.execute_input":"2024-06-10T03:07:51.576551Z","iopub.status.idle":"2024-06-10T03:07:51.976906Z","shell.execute_reply.started":"2024-06-10T03:07:51.576516Z","shell.execute_reply":"2024-06-10T03:07:51.976141Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/sacrebleu/sacrebleu.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Data collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n# Training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"steps\",\n    eval_steps = 1000,\n    save_strategy=\"steps\", # Change to \"steps\" to save after a certain number of steps\n    save_steps=1000, # Save after every 1000 steps\n    learning_rate=1e-5,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=1,\n    predict_with_generate=True,\n    fp16 = True\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T03:11:07.200086Z","iopub.execute_input":"2024-06-10T03:11:07.200593Z","iopub.status.idle":"2024-06-10T03:11:07.237213Z","shell.execute_reply.started":"2024-06-10T03:11:07.200561Z","shell.execute_reply":"2024-06-10T03:11:07.236417Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\nwandb.login(key = \"657caa4a9ec74a7425c69683dc166f64282e7513\")\nwandb.init(project = \"MT\")\n# Train the model\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model\ntrainer.save_model(\"./fine-tuned-vit5\")\nprint(\"Model saved\")","metadata":{"execution":{"iopub.status.busy":"2024-06-10T03:06:58.696201Z","iopub.status.idle":"2024-06-10T03:06:58.696659Z","shell.execute_reply.started":"2024-06-10T03:06:58.696436Z","shell.execute_reply":"2024-06-10T03:06:58.696455Z"},"trusted":true},"execution_count":null,"outputs":[]}]}