{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSGZJrlzc0Z1"
      },
      "source": [
        "BERT: encoding, predicts words based on surrounding context.\n",
        "\n",
        "GPT: decoding, uses context from left to right to predict the next word in a sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cro1MseDH0Z_"
      },
      "source": [
        "## **Checkpoint**\n",
        "https://drive.google.com/drive/folders/1ii_lPm2-1CfIhQM8RVzLgTHMxXDKgnk4?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvXEN84Yk8ps"
      },
      "source": [
        "#**Low-resource Machine Translation using mBART50**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPhRyDa7kjoM"
      },
      "outputs": [],
      "source": [
        "# !pip install -q transformers sentencepiece datasets accelerate evaluate sacrebleu\n",
        "# # sentencepiece to encode,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0VZzEh-l3-k"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaZ1uLwqBgZJ"
      },
      "outputs": [],
      "source": [
        "## Config\n",
        "# Định nghĩa lớp cấu hình cơ bản\n",
        "class BaseConfig:\n",
        "    \"\"\" base Encoder Decoder config \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "# Định nghĩa lớp cấu hình cho bài toán dịch thuật\n",
        "class NMTConfig(BaseConfig):\n",
        "    # Data\n",
        "    src_lang = 'en'  # Ngôn ngữ nguồn\n",
        "    tgt_lang = 'vi'  # Ngôn ngữ đích\n",
        "    max_len = 75  # Độ dài tối đa của câu\n",
        "    add_special_tokens = True  # Thêm các token đặc biệt\n",
        "\n",
        "    # Model\n",
        "    model_name = \"facebook/mbart-large-50-many-to-many-mmt\"  # Tên mô hình\n",
        "\n",
        "    # Training\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')  # Thiết bị (GPU hoặc CPU)\n",
        "    learning_rate = 5e-5  # Tốc độ học\n",
        "    train_batch_size = 16  # Kích thước batch khi huấn luyện\n",
        "    eval_batch_size = 16  # Kích thước batch khi đánh giá\n",
        "    num_train_epochs = 2  # Số epoch huấn luyện\n",
        "    save_total_limit = 1  # Số lượng checkpoint lưu trữ\n",
        "    ckpt_dir = f'./mbart50-{src_lang}-{tgt_lang}'  # Đường dẫn lưu trữ checkpoint\n",
        "    eval_steps = 1000  # Số bước mỗi lần đánh giá\n",
        "\n",
        "    # Inference\n",
        "    beam_size = 5  # Kích thước beam search\n",
        "\n",
        "cfg = NMTConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "id": "lqSbJ8aMFb9E",
        "outputId": "81a9cabc-ab4e-4add-9080-c9b947cb6bed"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Định nghĩa tên model: Bài này mình xài chung model này làm model và tokenizer\n",
        "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "\n",
        "# Tokenizer and add to cfg.tokenizer\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name, src_lang=\"en_XX\", tgt_lang=\"vi_VN\")\n",
        "cfg.tokenizer = MBart50TokenizerFast.from_pretrained(cfg.model_name)\n",
        "\n",
        "# Định nghĩa tên mô hình và tải tokenizer từ hugging\n",
        "MBart50_model = AutoModelForSeq2SeqLM.from_pretrained(cfg.model_name)\n",
        "\n",
        "\n",
        "# Tải mô hình từ checkpoint\n",
        "drive.mount('/content/drive/') # Mount Google Drive\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/colab/model/mbart50-en-vi/checkpoint-16000\"\n",
        "mbart50_envi_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)\n",
        "\n",
        "\n",
        "# checkpoint_path = \"/content/drive/MyDrive/colab/model/mbart50-en-vi-backtranslation/checkpoint-9000\"\n",
        "# mbart50_envi_backtranslation_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "ypwCCI0ZNhLq",
        "outputId": "79360920-3a43-4371-ef3b-962539a52b61"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgPXetxoFgEw"
      },
      "outputs": [],
      "source": [
        "def inference(\n",
        "    text,\n",
        "    tokenizer,\n",
        "    model,\n",
        "    device=\"cpu\",\n",
        "    max_length=75,\n",
        "    beam_size=5\n",
        "    ):\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "        )\n",
        "    input_ids = inputs.input_ids.to(device)\n",
        "    attention_mask = inputs.attention_mask.to(device)\n",
        "    model.to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=max_length,\n",
        "        early_stopping=True,\n",
        "        num_beams=beam_size,\n",
        "        length_penalty=2.0\n",
        "    )\n",
        "\n",
        "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    return output_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cR2g4McqFivB"
      },
      "outputs": [],
      "source": [
        "sentence = 'i go to school'\n",
        "inference(sentence, cfg.tokenizer, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dzobIFVH2J9"
      },
      "source": [
        "# Tính toán BLEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJI40YDqIE-X"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import torch\n",
        "\n",
        "# Define sentences to test and their references\n",
        "sentences = [\n",
        "    'i go to school',\n",
        "    'she loves programming',\n",
        "    'he is reading a book'\n",
        "]\n",
        "references = [\n",
        "    ['tôi đi học'],\n",
        "    ['cô ấy yêu lập trình'],\n",
        "    ['anh ấy đang đọc sách']\n",
        "]\n",
        "\n",
        "# Load BLEU metric\n",
        "bleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "# Function to evaluate a model and print BLEU score\n",
        "def evaluate_model(model, model_name, device=\"cpu\"):\n",
        "    model.to(device)\n",
        "    predictions = [inference(sentence, cfg.tokenizer, model, device=device)[0] for sentence in sentences]\n",
        "    results = bleu.compute(predictions=predictions, references=references)\n",
        "    print(f\"BLEU score for {model_name}: {results['score']}\")\n",
        "    print(f\"Predictions: {predictions}\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Evaluate mbart50_envi_model\n",
        "evaluate_model(mbart50_envi_model, \"mbart50_envi_model\", device)\n",
        "\n",
        "# Evaluate MBart50_model\n",
        "evaluate_model(MBart50_model, \"MBart50_model\", device)\n",
        "\n",
        "# Evaluate mbart50_envi_backtranslation_model\n",
        "evaluate_model(mbart50_envi_backtranslation_model, \"mbart50_envi_backtranslation_model\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0RQ_E3lILyX"
      },
      "outputs": [],
      "source": [
        "# Đánh giá trên tập test.en, test.vi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAKZTDlAITl0",
        "outputId": "bb3d0307-e35e-49b9-9dc4-251e1e22971b"
      },
      "outputs": [],
      "source": [
        "# Cài đặt thư viện Kaggle\n",
        "!pip install kaggle\n",
        "\n",
        "# Tạo thư mục để lưu trữ kaggle.json\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# Di chuyển tệp kaggle.json vào thư mục .kaggle và cấp quyền\n",
        "!mv /content/drive/MyDrive/colab/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Kiểm tra sự tồn tại của tệp kaggle.json\n",
        "!ls ~/.kaggle/\n",
        "\n",
        "# Kiểm tra nội dung của tệp kaggle.json (chỉ kiểm tra trong môi trường an toàn)\n",
        "!cat ~/.kaggle/kaggle.json\n",
        "\n",
        "# Kiểm tra quyền truy cập của tệp kaggle.json\n",
        "!ls -l ~/.kaggle/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPdvp9GqIxQX",
        "outputId": "c4fb410e-83fb-4003-c90a-c89d80ce262e"
      },
      "outputs": [],
      "source": [
        "# Tải dữ liệu từ Kaggle\n",
        "!kaggle datasets download -d cngonngc/phomt-datasetvinai\n",
        "\n",
        "# Giải nén tệp dữ liệu đã tải về\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('phomt-datasetvinai.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('phomt-datasetvinai')\n",
        "\n",
        "# Kiểm tra các tệp đã được giải nén\n",
        "!ls phomt-datasetvinai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrHUXa24Ji5N",
        "outputId": "6c03cf33-2ee7-4d4b-ab36-478ae717b050"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Định nghĩa đường dẫn tới các tệp test\n",
        "test_en_path = 'phomt-datasetvinai/test.en'\n",
        "test_vi_path = 'phomt-datasetvinai/test.vi'\n",
        "\n",
        "# Đọc dữ liệu từ các tệp\n",
        "with open(test_en_path, 'r') as f:\n",
        "    test_sentences_en = f.readlines()\n",
        "\n",
        "with open(test_vi_path, 'r') as f:\n",
        "    test_sentences_vi = f.readlines()\n",
        "\n",
        "# Lựa chọn một mẫu nhỏ để kiểm tra\n",
        "sample_size = 10  # Chọn 10 câu để kiểm tra\n",
        "sample_en = test_sentences_en[:sample_size]\n",
        "sample_vi = test_sentences_vi[:sample_size]\n",
        "\n",
        "# Generate translations\n",
        "predictions = [inference(sentence.strip(), cfg.tokenizer, model)[0] for sentence in sample_en]\n",
        "\n",
        "# Load BLEU metric\n",
        "bleu = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "# Compute BLEU score\n",
        "results = bleu.compute(predictions=predictions, references=[[ref.strip()] for ref in sample_vi])\n",
        "\n",
        "print(f\"BLEU score: {results['score']}\")\n",
        "print(f\"Predictions: {predictions[:5]}\")\n",
        "print(f\"References: {sample_vi[:5]}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
